---
content_type: page
title: Lecture Notes
uid: 8f6c4243-744a-9b41-6623-a021d0178db2
---

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
Lec #
{{< thclose >}}
{{< thopen >}}
Learning Objectives
{{< thclose >}}
{{< thopen >}}
Lecture Notes
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen >}}
1
{{< tdclose >}}
{{< tdopen >}}


*   To understand how the timescale of diffusion relates to length scales
*   To understand how concentration gradients lead to currents (Fick’s First Law)
*   To understand how charge drift in an electric field leads to currents (Ohm’s Law and resistivity)


{{< tdclose >}}
{{< tdopen >}}
[Overview and Ionic Currents (PDF - 1.7MB)]({{< baseurl >}}/resources/mit9_40s18_lec01)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
2
{{< tdclose >}}
{{< tdopen >}}


*   To understand how neurons respond to injected currents
*   To understand how membrane capacitance and resistance allows neurons to integrate or smooth their inputs over time (RC model)
*   To understand how to derive the differential equations for the RC model
*   To be able to sketch the response of an RC neuron to different current inputs
*   To understand where the ‘batteries’ of a neuron come from


{{< tdclose >}}
{{< tdopen >}}
[RC Circuit and Nernst Potential (PDF - 2.7MB)]({{< baseurl >}}/resources/mit9_40s18_lec02)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
3
{{< tdclose >}}
{{< tdopen >}}


*   To be able to construct a simplified model neuron by replacing the complex spike generating mechanisms of the real neuron (HH model) with a simplified spike generating mechanism
*   To understand the processes that neurons spend most of their time doing which is integrating inputs in the interval between spikes
*   To be able to create a quantitative description of the firing rate of neurons in response to current inputs
*   To provide an easy-to implement model that captures the basic properties of spiking neurons


{{< tdclose >}}
{{< tdopen >}}
[Nernst Potential and Integrate and Fire Models​ (PDF - 4.1MB)]({{< baseurl >}}/resources/mit9_40s18_lec03)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
4
{{< tdclose >}}
{{< tdopen >}}


*   To be able to draw the circuit diagram of the HH model
*   Understand what a voltage clamp is and how it works
*   Be able to plot the voltage and time dependence of the potassium current and conductance
*   Be able to explain the time and voltage dependence of the potassium conductance in terms of Hodgkin-Huxley gating variables


{{< tdclose >}}
{{< tdopen >}}
[Hodgkin Huxley Model Part 1 (PDF - 6.3MB)]({{< baseurl >}}/resources/mit9_40s18_lec04)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
5
{{< tdclose >}}
{{< tdopen >}}
[Hodgkin Huxley Model Part 2 (PDF - 3.3MB)]({{< baseurl >}}/resources/mit9_40s18_lec05)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
6
{{< tdclose >}}
{{< tdopen >}}


*   To be able to draw the ‘circuit diagram’ of a dendrite
*   Be able to plot the voltage in a dendrite as a function of distance for leaky and non-leaky dendrite, and understand the concept of a length constant
*   Know how length constant depends on dendritic radius
*   Understand the concept of electrotonic length
*   Be able to draw the circuit diagram a two-compartment model


{{< tdclose >}}
{{< tdopen >}}
[Dendrites (PDF - 3.2MB)]({{< baseurl >}}/resources/mit9_40s18_lec06)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
7
{{< tdclose >}}
{{< tdopen >}}


*   Be able to add a synapse in an equivalent circuit model
*   To describe a simple model of synaptic transmission
*   To be able to describe synaptic transmission as a convolution of a linear kernel with a spike train
*   To understand synaptic saturation
*   To understand the different functions of somatic and dendritic inhibition


{{< tdclose >}}
{{< tdopen >}}
[Synapses (PDF - 3.1MB)]({{< baseurl >}}/resources/mit9_40s18_lec07)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
8
{{< tdclose >}}
{{< tdopen >}}


*   To understand the origin of extracellular spike waveforms and local field potentials
*   To understand how to extract local field potentials and spike signals by low-pass and high-pass filtering, respectively
*   To be able to extract spike times as a threshold crossing
*   To understand what a peri-stimulus time histogram (PSTH) and a tuning curve is
*   To know how to compute the firing rate of a neuron by smoothing a spike train


{{< tdclose >}}
{{< tdopen >}}
[Spike Trains (PDF - 2.6MB)]({{< baseurl >}}/resources/mit9_40s18_lec08)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
9
{{< tdclose >}}
{{< tdopen >}}


*   To be able to mathematically describe a neural response as a linear filter followed by a nonlinear function.
    *   A correlation of a spatial receptive field with the stimulus
    *   A convolution of a temporal receptive field with the stimulus
*   To understand the concept of a Spatio-temporal Receptive Field (STRF) and the concept of ‘separability’
*   To understand the idea of a Spike Triggered Average and how to use it to compute a Spatio-temporal Receptive Field and a Spectro-temporal Receptive Field (STRF).


{{< tdclose >}}
{{< tdopen >}}
[Receptive Fields (PDF - 2.1MB)]({{< baseurl >}}/resources/mit9_40s18_lec09)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
10
{{< tdclose >}}
{{< tdopen >}}


*   Spike trains are probabilistic (Poisson Process)
*   Be able to use measures of spike train variability
    *   Fano Factor
    *   Interspike Interval (ISI)
*   Understand convolution, cross-correlation, and autocorrelation functions
*   Understand the concept of a Fourier series


{{< tdclose >}}
{{< tdopen >}}
[Time Series (PDF - 4.5MB)]({{< baseurl >}}/resources/mit9_40s18_lec10)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
11
{{< tdclose >}}
{{< tdopen >}}


*   Fourier series for symmetric and asymmetric functions
*   Complex Fourier series
*   Fourier transform
*   Discrete Fourier transform (Fast Fourier Transform - FFT)
*   Power spectrum


{{< tdclose >}}
{{< tdopen >}}
[Spectral Analysis Part 1 (PDF - 4.3MB)]({{< baseurl >}}/resources/mit9_40s18_lec11)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
12
{{< tdclose >}}
{{< tdopen >}}


*   Fourier Transform Pairs
*   Convolution Theorem
*   Gaussian Noise (Fourier Transform and Power Spectrum)
*   Spectral Estimation
    *   Filtering in the frequency domain
    *   Wiener-Kinchine Theorem
*   Shannon-Nyquist Theorem (and zero padding)
*   Line noise removal


{{< tdclose >}}
{{< tdopen >}}
[Spectral Analysis Part 2 (PDF - 3.1MB)]({{< baseurl >}}/resources/mit9_40s18_lec12)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
13
{{< tdclose >}}
{{< tdopen >}}


*   Brief review of Fourier transform pairs and convolution theorem
*   Spectral estimation
    *   Windows and Tapers
*   Spectrograms
*   Multi-taper spectral analysis
    *   How to design the best tapers (DPSS)
    *   Controlling the time-bandwith product
*   Advanced filtering methods


{{< tdclose >}}
{{< tdopen >}}
[Spectral Analysis Part 3 (PDF - 2.2MB)]({{< baseurl >}}/resources/mit9_40s18_lec13)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
14
{{< tdclose >}}
{{< tdopen >}}


*   Derive a mathematically tractable model of neural networks (the rate model)
*   Building receptive fields with neural networks
*   Vector notation and vector algebra
*   Neural networks for classification
*   Perceptrons


{{< tdclose >}}
{{< tdopen >}}
[Rate Models and Perceptrons (PDF - 3.9MB)]({{< baseurl >}}/resources/mit9_40s18_lec14)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
15
{{< tdclose >}}
{{< tdopen >}}


*   Perceptrons and perceptron learning rule
*   Neuronal logic, linear separability, and invariance
*   Two-layer feedforward networks
*   Matrix algebra review
*   Matrix transformations


{{< tdclose >}}
{{< tdopen >}}
[Matrix Operations (PDF - 4.0MB)]({{< baseurl >}}/resources/mit9_40s18_lec15)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
16
{{< tdclose >}}
{{< tdopen >}}


*   More on two-layer feed-forward networks
*   Matrix transformations (rotated transformations)
*   Basis sets
*   Linear independence
*   Change of basis


{{< tdclose >}}
{{< tdopen >}}
[Basis Sets (PDF - 2.8MB)]({{< baseurl >}}/resources/mit9_40s18_lec16)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
17
{{< tdclose >}}
{{< tdopen >}}


*   Eigenvectors and eigenvalues
*   Variance and multivariate Gaussian distributions
*   Computing a covariance matrix from data
*   Principal Components Analysis (PCA)


{{< tdclose >}}
{{< tdopen >}}
[Principal Components Analysis​ (PDF - 4.8MB)]({{< baseurl >}}/resources/mit9_40s18_lec17)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
18
{{< tdclose >}}
{{< tdopen >}}


*   Mathematical description of recurrent networks
*   Dynamics in simple autapse networks
*   Dynamics in fully recurrent networks
*   Recurrent networks for storing memories
*   Recurrent networks for decision making (winner-take-all)


{{< tdclose >}}
{{< tdopen >}}
[Recurrent Networks (PDF - 2.2MB)]({{< baseurl >}}/resources/mit9_40s18_lec18)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
19
{{< tdclose >}}
{{< tdopen >}}


*   Recurrent neural networks and memory
*   The oculomotor system as a model of short term memory and neural integration
*   Stability in neural integrators
*   Learning in neural integrators


{{< tdclose >}}
{{< tdopen >}}
[Neural Integrators (PDF - 2.0MB)]({{< baseurl >}}/resources/mit9_40s18_lec19)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
20
{{< tdclose >}}
{{< tdopen >}}


*   Recurrent networks with lambda greater than one
    *   Attractors
*   Winner-take-all networks
*   Attractor networks for long-term memory (Hopfield model)
*   Energy landscape
*   Hopfield network capacity


{{< tdclose >}}
{{< tdopen >}}
[Hopfield Networks (PDF - 2.7MB)]({{< baseurl >}}/resources/mit9_40s18_lec20)
{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}